{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "interior-grass",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import Resize, Compose, ToTensor, Normalize\n",
    "import random\n",
    "# from scipy.misc import imread\n",
    "from imageio import imread\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "# from fasterRCNN.lib.model.utils.blob import prep_im_for_blob, im_list_to_blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "prerequisite-sociology",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MG(Dataset):\n",
    "\n",
    "    def __init__(self, mode, datasize, data_path=None, filter_nonperson_box_frame=True, filter_small_box=False):\n",
    "\n",
    "        root_path = data_path\n",
    "        self.frames_path = os.path.join(root_path, 'frames/')\n",
    "\n",
    "        # collect the object classes\n",
    "        self.object_classes = ['__background__']\n",
    "        with open(os.path.join(root_path, 'annotations/object_classes.txt'), 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                line = line.strip('\\n')\n",
    "                self.object_classes.append(line)\n",
    "        f.close()\n",
    "        self.object_classes[9] = 'closet/cabinet'\n",
    "        self.object_classes[11] = 'cup/glass/bottle'\n",
    "        self.object_classes[23] = 'paper/notebook'\n",
    "        self.object_classes[24] = 'phone/camera'\n",
    "        self.object_classes[31] = 'sofa/couch'\n",
    "\n",
    "        # collect relationship classes\n",
    "        self.relationship_classes = []\n",
    "        with open(os.path.join(root_path, 'annotations/relationship_classes.txt'), 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                line = line.strip('\\n')\n",
    "                self.relationship_classes.append(line)\n",
    "        f.close()\n",
    "        self.relationship_classes[0] = 'looking_at'\n",
    "        self.relationship_classes[1] = 'not_looking_at'\n",
    "        self.relationship_classes[5] = 'in_front_of'\n",
    "        self.relationship_classes[7] = 'on_the_side_of'\n",
    "        self.relationship_classes[10] = 'covered_by'\n",
    "        self.relationship_classes[11] = 'drinking_from'\n",
    "        self.relationship_classes[13] = 'have_it_on_the_back'\n",
    "        self.relationship_classes[15] = 'leaning_on'\n",
    "        self.relationship_classes[16] = 'lying_on'\n",
    "        self.relationship_classes[17] = 'not_contacting'\n",
    "        self.relationship_classes[18] = 'other_relationship'\n",
    "        self.relationship_classes[19] = 'sitting_on'\n",
    "        self.relationship_classes[20] = 'standing_on'\n",
    "        self.relationship_classes[25] = 'writing_on'\n",
    "\n",
    "        self.attention_relationships = self.relationship_classes[0:3]\n",
    "        self.spatial_relationships = self.relationship_classes[3:9]\n",
    "        self.contacting_relationships = self.relationship_classes[9:]\n",
    "\n",
    "\n",
    "#         print('-------loading annotations---------slowly-----------')\n",
    "\n",
    "#         if filter_small_box:\n",
    "#             with open(root_path + 'annotations/person_bbox.pkl', 'rb') as f:\n",
    "#                 person_bbox = pickle.load(f)\n",
    "#             f.close()\n",
    "#             with open('dataloader/object_bbox_and_relationship_filtersmall.pkl', 'rb') as f:\n",
    "#                 object_bbox = pickle.load(f)\n",
    "#         else:\n",
    "#             with open(root_path + 'annotations/person_bbox.pkl', 'rb') as f:\n",
    "#                 person_bbox = pickle.load(f)\n",
    "#             f.close()\n",
    "#             with open(root_path+'annotations/object_bbox_and_relationship.pkl', 'rb') as f:\n",
    "#                 object_bbox = pickle.load(f)\n",
    "#             f.close()\n",
    "#         print('--------------------finish!-------------------------')\n",
    "\n",
    "#         if datasize == 'mini':\n",
    "#             small_person = {}\n",
    "#             small_object = {}\n",
    "#             for i in list(person_bbox.keys())[:80000]:\n",
    "#                 small_person[i] = person_bbox[i]\n",
    "#                 small_object[i] = object_bbox[i]\n",
    "#             person_bbox = small_person\n",
    "#             object_bbox = small_object\n",
    "\n",
    "\n",
    "        # collect valid frames\n",
    "        video_dict = {}\n",
    "        for video_name in os.listdir(root_path + \"videos\"):\n",
    "#             print(video_name)\n",
    "            path_to_frames = os.path.join(root_path, \"frames\", video_name)\n",
    "            for frame_path in os.listdir(os.path.join(root_path, \"frames\", video_name)):\n",
    "                if video_name in video_dict.keys():\n",
    "                    video_dict[video_name].append(frame_path)\n",
    "                else:\n",
    "                    video_dict[video_name] = [frame_path]\n",
    "            print(video_name)\n",
    "            print(video_dict[video_name])\n",
    "            \n",
    "#         for i in person_bbox.keys():\n",
    "#             if object_bbox[i][0]['metadata']['set'] == mode: #train or testing?\n",
    "#                 frame_valid = False\n",
    "#                 for j in object_bbox[i]: # the frame is valid if there is visible bbox\n",
    "#                     if j['visible']:\n",
    "#                         frame_valid = True\n",
    "#                 if frame_valid:\n",
    "#                     video_name, frame_num = i.split('/')\n",
    "#                     if video_name in video_dict.keys():\n",
    "#                         video_dict[video_name].append(i)\n",
    "#                     else:\n",
    "#                         video_dict[video_name] = [i]\n",
    "\n",
    "        self.video_list = []\n",
    "        self.video_size = [] # (w,h)\n",
    "        self.gt_annotations = []\n",
    "        self.non_gt_human_nums = 0\n",
    "        self.non_heatmap_nums = 0\n",
    "        self.non_person_video = 0\n",
    "        self.one_frame_video = 0\n",
    "        self.valid_nums = 0\n",
    "\n",
    "        '''\n",
    "        filter_nonperson_box_frame = True (default): according to the stanford method, remove the frames without person box both for training and testing\n",
    "        filter_nonperson_box_frame = False: still use the frames without person box, FasterRCNN may find the person\n",
    "        '''\n",
    "        print(video_dict.keys())\n",
    "        for i in video_dict.keys():\n",
    "            video = []\n",
    "            gt_annotation_video = []\n",
    "            for j in video_dict[i]:\n",
    "                if filter_nonperson_box_frame:\n",
    "                    if person_bbox[j]['bbox'].shape[0] == 0:\n",
    "                        self.non_gt_human_nums += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        video.append(j)\n",
    "                        self.valid_nums += 1\n",
    "\n",
    "#                 print(person_bbox)\n",
    "#                 gt_annotation_frame = [{'person_bbox': person_bbox[j]['bbox']}]\n",
    "                # each frames's objects and human\n",
    "#                 for k in object_bbox[j]:\n",
    "#                     if k['visible']:\n",
    "#                         assert k['bbox'] != None, 'warning! The object is visible without bbox'\n",
    "#                         k['class'] = self.object_classes.index(k['class'])\n",
    "#                         k['bbox'] = np.array([k['bbox'][0], k['bbox'][1], k['bbox'][0]+k['bbox'][2], k['bbox'][1]+k['bbox'][3]]) # from xywh to xyxy\n",
    "#                         k['attention_relationship'] = torch.tensor([self.attention_relationships.index(r) for r in k['attention_relationship']], dtype=torch.long)\n",
    "#                         k['spatial_relationship'] = torch.tensor([self.spatial_relationships.index(r) for r in k['spatial_relationship']], dtype=torch.long)\n",
    "#                         k['contacting_relationship'] = torch.tensor([self.contacting_relationships.index(r) for r in k['contacting_relationship']], dtype=torch.long)\n",
    "#                         gt_annotation_frame.append(k)\n",
    "#                 gt_annotation_video.append(gt_annotation_frame)\n",
    "\n",
    "            if len(video) > 2:\n",
    "                self.video_list.append(video)\n",
    "                self.video_size.append(person_bbox[j]['bbox_size'])\n",
    "                self.gt_annotations.append(gt_annotation_video)\n",
    "            elif len(video) == 1:\n",
    "                self.one_frame_video += 1\n",
    "            else:\n",
    "                self.non_person_video += 1\n",
    "\n",
    "        print('x'*60)\n",
    "        if filter_nonperson_box_frame:\n",
    "            print('There are {} videos and {} valid frames'.format(len(self.video_list), self.valid_nums))\n",
    "            print('{} videos are invalid (no person), remove them'.format(self.non_person_video))\n",
    "            print('{} videos are invalid (only one frame), remove them'.format(self.one_frame_video))\n",
    "            print('{} frames have no human bbox in GT, remove them!'.format(self.non_gt_human_nums))\n",
    "        else:\n",
    "            print('There are {} videos and {} valid frames'.format(len(self.video_list), self.valid_nums))\n",
    "            print('{} frames have no human bbox in GT'.format(self.non_gt_human_nums))\n",
    "            print('Removed {} of them without joint heatmaps which means FasterRCNN also cannot find the human'.format(non_heatmap_nums))\n",
    "        print('x' * 60)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        frame_names = self.video_list[index]\n",
    "        processed_ims = []\n",
    "        im_scales = []\n",
    "\n",
    "        for idx, name in enumerate(frame_names):\n",
    "            im = imread(os.path.join(self.frames_path, name)) # channel h,w,3\n",
    "            im = im[:, :, ::-1] # rgb -> bgr\n",
    "            im, im_scale = prep_im_for_blob(im, [[[102.9801, 115.9465, 122.7717]]], 600, 1000) #cfg.PIXEL_MEANS, target_size, cfg.TRAIN.MAX_SIZE\n",
    "            im_scales.append(im_scale)\n",
    "            processed_ims.append(im)\n",
    "\n",
    "        blob = im_list_to_blob(processed_ims)\n",
    "        im_info = np.array([[blob.shape[1], blob.shape[2], im_scales[0]]],dtype=np.float32)\n",
    "        im_info = torch.from_numpy(im_info).repeat(blob.shape[0], 1)\n",
    "        img_tensor = torch.from_numpy(blob)\n",
    "        img_tensor = img_tensor.permute(0, 3, 1, 2)\n",
    "\n",
    "        gt_boxes = torch.zeros([img_tensor.shape[0], 1, 5])\n",
    "        num_boxes = torch.zeros([img_tensor.shape[0]], dtype=torch.int64)\n",
    "\n",
    "        return img_tensor, im_info, gt_boxes, num_boxes, index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "difficult-allergy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cuda_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    don't need to zip the tensor\n",
    "\n",
    "    \"\"\"\n",
    "    return batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "governmental-rwanda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['scene_43_valid.mp4', 'scene_17_valid.mp4', 'scene_6_valid.mp4', 'scene_34_valid.mp4', 'scene_51_valid.mp4', 'scene_7_valid.mp4', 'scene_19_valid.mp4', 'scene_64_valid.mp4', 'scene_46_valid.mp4', 'scene_62_valid.mp4', 'scene_39_invalid.mp4', 'scene_20_valid.mp4', 'scene_53_valid.mp4', 'scene_11_valid.mp4', 'scene_15_valid.mp4', 'scene_67_valid.mp4', 'scene_37_valid.mp4', 'scene_12_valid.mp4', 'scene_32_valid.mp4', 'scene_35_valid.mp4', 'scene_44_valid.mp4', 'scene_4_valid.mp4', 'scene_27_valid.mp4', 'scene_66_valid.mp4', 'scene_26_valid.mp4', 'scene_8_valid.mp4', 'scene_31_valid.mp4', 'scene_21_valid.mp4', 'scene_22_valid.mp4', 'scene_48_valid.mp4', 'scene_28_valid.mp4', 'scene_60_valid.mp4', 'scene_49_valid.mp4', 'scene_1_invalid.mp4', 'scene_10_valid.mp4', 'scene_59_valid.mp4', 'scene_63_valid.mp4', 'scene_56_valid.mp4', 'scene_38_valid.mp4', 'scene_65_valid.mp4', 'scene_14_valid.mp4', 'scene_54_valid.mp4', 'scene_3_valid.mp4', 'scene_18_valid.mp4', 'scene_29_valid.mp4', 'scene_2_invalid.mp4', 'scene_5_valid.mp4', 'scene_58_valid.mp4', 'scene_25_valid.mp4', 'scene_42_valid.mp4', 'scene_13_valid.mp4', 'scene_55_valid.mp4', 'scene_24_valid.mp4', 'scene_45_valid.mp4', 'scene_41_valid.mp4', 'scene_9_valid.mp4', 'scene_47_valid.mp4', 'scene_57_valid.mp4', 'scene_23_valid.mp4', 'scene_36_valid.mp4', 'scene_61_valid.mp4', 'scene_50_valid.mp4', 'scene_30_valid.mp4', 'scene_33_valid.mp4', 'scene_52_valid.mp4', 'scene_16_valid.mp4', 'scene_40_valid.mp4'])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'object_bbox' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-8119667918b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#                 filter_small_box=False if mode == 'predcls' else True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mMG_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmg_data_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_nonperson_box_frame\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_small_box\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'predcls'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# AG_dataset = AG(mode=\"test\", datasize=conf.datasize, data_path=conf.data_path, filter_nonperson_box_frame=True,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-438d8f471e2b>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, mode, datasize, data_path, filter_nonperson_box_frame, filter_small_box)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;31m#                 gt_annotation_frame = [{'person_bbox': person_bbox[j]['bbox']}]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0;31m# each frames's objects and human\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobject_bbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'visible'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                         \u001b[0;32massert\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bbox'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'warning! The object is visible without bbox'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'object_bbox' is not defined"
     ]
    }
   ],
   "source": [
    "# conf = Config()\n",
    "# print(conf)\n",
    "# for i in conf.args:\n",
    "#     print(i,':', conf.args[i])\n",
    "datasize = 'large'\n",
    "ag_data_path = \"../../ActionGenome/dataset/ag/\"\n",
    "mg_data_path = \"../../TER_MovieGraph/scene_library/\"\n",
    "mode = 'sgdet'\n",
    "\n",
    "\n",
    "# AG_dataset = AG(mode=\"test\", datasize=datasize, data_path=ag_data_path, filter_nonperson_box_frame=True, \n",
    "#                 filter_small_box=False if mode == 'predcls' else True)\n",
    "\n",
    "MG_dataset = MG(mode =\"test\", datasize = datasize, data_path = mg_data_path, filter_nonperson_box_frame=False, filter_small_box=False if mode == 'predcls' else True)\n",
    "\n",
    "# AG_dataset = AG(mode=\"test\", datasize=conf.datasize, data_path=conf.data_path, filter_nonperson_box_frame=True, \n",
    "#                 filter_small_box=False if conf.mode == 'predcls' else True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-monitoring",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
