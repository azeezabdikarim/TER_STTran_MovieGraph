{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "functional-consent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: This code was developed and tested with NetworkX 1.11, but you are using version 2.6.3.\n",
      "networkx-2 will NOT work because they changed many functions!\n",
      "/nfshome/students/aa211327/TER-SceneGraphGeneration/STTran_clean\n",
      "Warning: This code was developed and tested with NetworkX 1.11, but you are using version 2.6.3.\n",
      "networkx-2 will NOT work because they changed many functions!\n",
      "Total number of clip graphs: 215\n",
      "Out of the 2221 total triplets constructed in for the movie, 0 have a predicate match with ActionGenome\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "moviegraph/GraphClasses.py:132: RuntimeWarning: Time node without proper assignment!\n",
      "  warnings.warn(\"Time node without proper assignment!\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# from GraphClasses import ClipGraph, MovieGraph\n",
    "from moviegraph.MGAnnotations import MGAnnotations\n",
    "import sys\n",
    "sys.path.append('moviegraph/')\n",
    "\n",
    "\n",
    "\n",
    "# with open('moviegraph/2017-11-02-51-7637_py3.pkl', 'rb') as fid:\n",
    "#     all_mg = pickle.load(fid, encoding='latin1')\n",
    "    \n",
    "annotations_path = \"moviegraph/2017-11-02-51-7637_py3.pkl\"\n",
    "annotations = MGAnnotations(annotations_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scientific-warner",
   "metadata": {},
   "source": [
    "### Make All Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "chubby-mexican",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=4)\n",
    "import copy\n",
    "import torch\n",
    "import importlib\n",
    "\n",
    "import dataloader\n",
    "from dataloader.action_genome import AG, cuda_collate_fn\n",
    "from dataloader.movie_graph import MG\n",
    "\n",
    "from lib.config import Config\n",
    "from lib.evaluation_recall import BasicSceneGraphEvaluator\n",
    "from lib.evaluation_recall_mg import MGSceneGraphEvaluator\n",
    "from lib.object_detector import detector\n",
    "from lib.sttran import STTran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "representative-knight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "detector(\n",
       "  (fasterRCNN): resnet(\n",
       "    (RCNN_rpn): _RPN(\n",
       "      (RPN_Conv): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (RPN_cls_score): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (RPN_bbox_pred): Conv2d(512, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (RPN_proposal): _ProposalLayer()\n",
       "      (RPN_anchor_target): _AnchorTargetLayer()\n",
       "    )\n",
       "    (RCNN_proposal_target): _ProposalTargetLayer()\n",
       "    (RCNN_roi_pool): ROIPool(output_size=(7, 7), spatial_scale=0.0625)\n",
       "    (RCNN_roi_align): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0)\n",
       "    (RCNN_base): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      (4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (6): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (7): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (8): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (9): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (10): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (11): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (12): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (13): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (14): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (15): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (16): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (17): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (18): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (19): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (20): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (21): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (22): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (RCNN_top): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (RCNN_cls_score): Linear(in_features=2048, out_features=37, bias=True)\n",
       "    (RCNN_bbox_pred): Linear(in_features=2048, out_features=148, bias=True)\n",
       "  )\n",
       "  (ROI_Align): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasize = 'large'\n",
    "ag_data_path = \"../ActionGenome/dataset/ag/\"\n",
    "mg_data_path = \"../TER_MovieGraph/scene_library/\"\n",
    "mode = 'test'\n",
    "\n",
    "\n",
    "MG_dataset = MG(mode =mode, datasize = datasize, data_path = mg_data_path, filter_nonperson_box_frame=False, filter_small_box=False if mode == 'predcls' else True)\n",
    "mg_dataloader = torch.utils.data.DataLoader(MG_dataset, shuffle=False, num_workers=0, collate_fn=cuda_collate_fn)\n",
    "\n",
    "\n",
    "gpu_device = torch.device('cuda:0')\n",
    "mode = 'sgdet'\n",
    "object_detector = detector(train=False, object_classes=MG_dataset.object_classes, use_SUPPLY=True, mode=mode).to(device=gpu_device)\n",
    "object_detector.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "gothic-massage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word vector location: data data\n",
      "loading word vectors from data/data/glove.6B.200d.pt\n",
      "word vector location: /home/cong/Dokumente/neural-motifs-master/data /hom\n",
      "loading word vectors from data//home/cong/Dokumente/neural-motifs-master/data/glove.6B.200d.pt\n",
      "__background__ -> __background__ \n",
      "fail on __background__\n",
      "**************************************************\n",
      "CKPT pretrained_models/sgdet.tar is loaded\n"
     ]
    }
   ],
   "source": [
    "enc_layer = 1\n",
    "dec_layer = 3\n",
    "model_path = \"pretrained_models/sgdet.tar\"\n",
    "\n",
    "model = STTran(mode=mode,\n",
    "               attention_class_num=len(MG_dataset.attention_relationships),\n",
    "               spatial_class_num=len(MG_dataset.spatial_relationships),\n",
    "               contact_class_num=len(MG_dataset.contacting_relationships),\n",
    "               obj_classes=MG_dataset.object_classes,\n",
    "               enc_layer_num=enc_layer,\n",
    "               dec_layer_num=dec_layer).to(device=gpu_device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "ckpt = torch.load(model_path, map_location=gpu_device)\n",
    "model.load_state_dict(ckpt['state_dict'], strict=False)\n",
    "print('*'*50)\n",
    "print('CKPT {} is loaded'.format(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "informed-convergence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction and Scene ID: 18\n",
      "torch.Size([12, 3, 600, 1064]) torch.Size([12, 3]) torch.Size([12, 1, 5]) torch.Size([12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:4737: UserWarning: Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\n",
      "  \"Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction and Scene ID: 14\n",
      "torch.Size([23, 3, 600, 1064]) torch.Size([23, 3]) torch.Size([23, 1, 5]) torch.Size([23])\n",
      "Prediction and Scene ID: 216\n",
      "torch.Size([18, 3, 600, 1064]) torch.Size([18, 3]) torch.Size([18, 1, 5]) torch.Size([18])\n",
      "Prediction and Scene ID: 163\n",
      "torch.Size([12, 3, 600, 1064]) torch.Size([12, 3]) torch.Size([12, 1, 5]) torch.Size([12])\n",
      "Prediction and Scene ID: 217\n",
      "torch.Size([6, 3, 600, 1064]) torch.Size([6, 3]) torch.Size([6, 1, 5]) torch.Size([6])\n",
      "Prediction and Scene ID: 213\n",
      "torch.Size([6, 3, 600, 1064]) torch.Size([6, 3]) torch.Size([6, 1, 5]) torch.Size([6])\n",
      "Prediction and Scene ID: 160\n",
      "torch.Size([26, 3, 600, 1064]) torch.Size([26, 3]) torch.Size([26, 1, 5]) torch.Size([26])\n",
      "Prediction and Scene ID: 11\n",
      "torch.Size([42, 3, 600, 1064]) torch.Size([42, 3]) torch.Size([42, 1, 5]) torch.Size([42])\n",
      "Prediction and Scene ID: 165\n",
      "torch.Size([22, 3, 600, 1064]) torch.Size([22, 3]) torch.Size([22, 1, 5]) torch.Size([22])\n",
      "Prediction and Scene ID: 103\n",
      "torch.Size([25, 3, 600, 1064]) torch.Size([25, 3]) torch.Size([25, 1, 5]) torch.Size([25])\n",
      "Prediction and Scene ID: 68\n",
      "torch.Size([73, 3, 600, 1064]) torch.Size([73, 3]) torch.Size([73, 1, 5]) torch.Size([73])\n",
      "Prediction and Scene ID: 221\n",
      "torch.Size([45, 3, 600, 1064]) torch.Size([45, 3]) torch.Size([45, 1, 5]) torch.Size([45])\n",
      "Prediction and Scene ID: 154\n",
      "torch.Size([21, 3, 600, 1064]) torch.Size([21, 3]) torch.Size([21, 1, 5]) torch.Size([21])\n",
      "Prediction and Scene ID: 209\n",
      "torch.Size([21, 3, 600, 1064]) torch.Size([21, 3]) torch.Size([21, 1, 5]) torch.Size([21])\n",
      "Prediction and Scene ID: 35\n",
      "torch.Size([46, 3, 600, 1064]) torch.Size([46, 3]) torch.Size([46, 1, 5]) torch.Size([46])\n",
      "Prediction and Scene ID: 28\n",
      "torch.Size([84, 3, 600, 1064]) torch.Size([84, 3]) torch.Size([84, 1, 5]) torch.Size([84])\n",
      "Prediction and Scene ID: 95\n",
      "torch.Size([58, 3, 600, 1064]) torch.Size([58, 3]) torch.Size([58, 1, 5]) torch.Size([58])\n",
      "Prediction and Scene ID: 117\n",
      "torch.Size([57, 3, 600, 1064]) torch.Size([57, 3]) torch.Size([57, 1, 5]) torch.Size([57])\n",
      "Prediction and Scene ID: 214\n",
      "torch.Size([6, 3, 600, 1064]) torch.Size([6, 3]) torch.Size([6, 1, 5]) torch.Size([6])\n",
      "Prediction and Scene ID: 98\n",
      "torch.Size([48, 3, 600, 1064]) torch.Size([48, 3]) torch.Size([48, 1, 5]) torch.Size([48])\n",
      "Prediction and Scene ID: 76\n",
      "torch.Size([24, 3, 600, 1064]) torch.Size([24, 3]) torch.Size([24, 1, 5]) torch.Size([24])\n",
      "Prediction and Scene ID: 203\n",
      "torch.Size([50, 3, 600, 1064]) torch.Size([50, 3]) torch.Size([50, 1, 5]) torch.Size([50])\n",
      "Prediction and Scene ID: 193\n",
      "torch.Size([17, 3, 600, 1064]) torch.Size([17, 3]) torch.Size([17, 1, 5]) torch.Size([17])\n",
      "Prediction and Scene ID: 137\n",
      "torch.Size([41, 3, 600, 1064]) torch.Size([41, 3]) torch.Size([41, 1, 5]) torch.Size([41])\n",
      "Prediction and Scene ID: 23\n",
      "torch.Size([17, 3, 600, 1064]) torch.Size([17, 3]) torch.Size([17, 1, 5]) torch.Size([17])\n",
      "Prediction and Scene ID: 66\n",
      "torch.Size([45, 3, 600, 1064]) torch.Size([45, 3]) torch.Size([45, 1, 5]) torch.Size([45])\n",
      "Prediction and Scene ID: 121\n",
      "torch.Size([28, 3, 600, 1064]) torch.Size([28, 3]) torch.Size([28, 1, 5]) torch.Size([28])\n",
      "Prediction and Scene ID: 87\n",
      "torch.Size([25, 3, 600, 1064]) torch.Size([25, 3]) torch.Size([25, 1, 5]) torch.Size([25])\n",
      "Prediction and Scene ID: 185\n",
      "torch.Size([88, 3, 600, 1064]) torch.Size([88, 3]) torch.Size([88, 1, 5]) torch.Size([88])\n",
      "Prediction and Scene ID: 192\n",
      "torch.Size([10, 3, 600, 1064]) torch.Size([10, 3]) torch.Size([10, 1, 5]) torch.Size([10])\n",
      "Prediction and Scene ID: 132\n",
      "torch.Size([105, 3, 600, 1064]) torch.Size([105, 3]) torch.Size([105, 1, 5]) torch.Size([105])\n",
      "Prediction and Scene ID: 157\n",
      "torch.Size([12, 3, 600, 1064]) torch.Size([12, 3]) torch.Size([12, 1, 5]) torch.Size([12])\n",
      "Prediction and Scene ID: 78\n",
      "torch.Size([38, 3, 600, 1064]) torch.Size([38, 3]) torch.Size([38, 1, 5]) torch.Size([38])\n",
      "Prediction and Scene ID: 109\n",
      "torch.Size([12, 3, 600, 1064]) torch.Size([12, 3]) torch.Size([12, 1, 5]) torch.Size([12])\n",
      "Prediction and Scene ID: 102\n",
      "torch.Size([23, 3, 600, 1064]) torch.Size([23, 3]) torch.Size([23, 1, 5]) torch.Size([23])\n",
      "Prediction and Scene ID: 99\n",
      "torch.Size([24, 3, 600, 1064]) torch.Size([24, 3]) torch.Size([24, 1, 5]) torch.Size([24])\n",
      "Prediction and Scene ID: 127\n",
      "torch.Size([26, 3, 600, 1064]) torch.Size([26, 3]) torch.Size([26, 1, 5]) torch.Size([26])\n",
      "Prediction and Scene ID: 114\n",
      "torch.Size([26, 3, 600, 1064]) torch.Size([26, 3]) torch.Size([26, 1, 5]) torch.Size([26])\n",
      "Prediction and Scene ID: 161\n",
      "torch.Size([59, 3, 600, 1064]) torch.Size([59, 3]) torch.Size([59, 1, 5]) torch.Size([59])\n",
      "Prediction and Scene ID: 113\n",
      "torch.Size([27, 3, 600, 1064]) torch.Size([27, 3]) torch.Size([27, 1, 5]) torch.Size([27])\n",
      "Prediction and Scene ID: 196\n",
      "torch.Size([21, 3, 600, 1064]) torch.Size([21, 3]) torch.Size([21, 1, 5]) torch.Size([21])\n",
      "Prediction and Scene ID: 15\n",
      "torch.Size([43, 3, 600, 1064]) torch.Size([43, 3]) torch.Size([43, 1, 5]) torch.Size([43])\n",
      "Prediction and Scene ID: 27\n",
      "torch.Size([61, 3, 600, 1064]) torch.Size([61, 3]) torch.Size([61, 1, 5]) torch.Size([61])\n",
      "Prediction and Scene ID: 143\n",
      "torch.Size([12, 3, 600, 1064]) torch.Size([12, 3]) torch.Size([12, 1, 5]) torch.Size([12])\n",
      "Prediction and Scene ID: 159\n",
      "torch.Size([40, 3, 600, 1064]) torch.Size([40, 3]) torch.Size([40, 1, 5]) torch.Size([40])\n",
      "Prediction and Scene ID: 17\n",
      "torch.Size([25, 3, 600, 1064]) torch.Size([25, 3]) torch.Size([25, 1, 5]) torch.Size([25])\n",
      "Prediction and Scene ID: 169\n",
      "torch.Size([18, 3, 600, 1064]) torch.Size([18, 3]) torch.Size([18, 1, 5]) torch.Size([18])\n",
      "Prediction and Scene ID: 40\n",
      "torch.Size([13, 3, 600, 1064]) torch.Size([13, 3]) torch.Size([13, 1, 5]) torch.Size([13])\n",
      "Prediction and Scene ID: 69\n",
      "torch.Size([47, 3, 600, 1064]) torch.Size([47, 3]) torch.Size([47, 1, 5]) torch.Size([47])\n",
      "Prediction and Scene ID: 34\n",
      "torch.Size([48, 3, 600, 1064]) torch.Size([48, 3]) torch.Size([48, 1, 5]) torch.Size([48])\n",
      "Prediction and Scene ID: 215\n",
      "torch.Size([14, 3, 600, 1064]) torch.Size([14, 3]) torch.Size([14, 1, 5]) torch.Size([14])\n",
      "Prediction and Scene ID: 48\n",
      "torch.Size([20, 3, 600, 1064]) torch.Size([20, 3]) torch.Size([20, 1, 5]) torch.Size([20])\n",
      "Prediction and Scene ID: 206\n",
      "torch.Size([37, 3, 600, 1064]) torch.Size([37, 3]) torch.Size([37, 1, 5]) torch.Size([37])\n",
      "Prediction and Scene ID: 115\n",
      "torch.Size([21, 3, 600, 1064]) torch.Size([21, 3]) torch.Size([21, 1, 5]) torch.Size([21])\n",
      "Prediction and Scene ID: 44\n",
      "torch.Size([18, 3, 600, 1064]) torch.Size([18, 3]) torch.Size([18, 1, 5]) torch.Size([18])\n",
      "Prediction and Scene ID: 111\n",
      "torch.Size([17, 3, 600, 1064]) torch.Size([17, 3]) torch.Size([17, 1, 5]) torch.Size([17])\n",
      "Prediction and Scene ID: 86\n",
      "torch.Size([7, 3, 600, 1064]) torch.Size([7, 3]) torch.Size([7, 1, 5]) torch.Size([7])\n",
      "Prediction and Scene ID: 31\n",
      "torch.Size([53, 3, 600, 1064]) torch.Size([53, 3]) torch.Size([53, 1, 5]) torch.Size([53])\n",
      "Prediction and Scene ID: 211\n",
      "torch.Size([18, 3, 600, 1064]) torch.Size([18, 3]) torch.Size([18, 1, 5]) torch.Size([18])\n",
      "Prediction and Scene ID: 91\n",
      "torch.Size([32, 3, 600, 1064]) torch.Size([32, 3]) torch.Size([32, 1, 5]) torch.Size([32])\n",
      "Prediction and Scene ID: 19\n",
      "torch.Size([31, 3, 600, 1064]) torch.Size([31, 3]) torch.Size([31, 1, 5]) torch.Size([31])\n",
      "Prediction and Scene ID: 149\n",
      "torch.Size([8, 3, 600, 1064]) torch.Size([8, 3]) torch.Size([8, 1, 5]) torch.Size([8])\n",
      "Prediction and Scene ID: 151\n",
      "torch.Size([32, 3, 600, 1064]) torch.Size([32, 3]) torch.Size([32, 1, 5]) torch.Size([32])\n",
      "Prediction and Scene ID: 45\n",
      "torch.Size([25, 3, 600, 1064]) torch.Size([25, 3]) torch.Size([25, 1, 5]) torch.Size([25])\n",
      "Prediction and Scene ID: 201\n",
      "torch.Size([20, 3, 600, 1064]) torch.Size([20, 3]) torch.Size([20, 1, 5]) torch.Size([20])\n",
      "Prediction and Scene ID: 166\n",
      "torch.Size([26, 3, 600, 1064]) torch.Size([26, 3]) torch.Size([26, 1, 5]) torch.Size([26])\n",
      "Prediction and Scene ID: 3\n",
      "torch.Size([34, 3, 600, 1064]) torch.Size([34, 3]) torch.Size([34, 1, 5]) torch.Size([34])\n",
      "Prediction and Scene ID: 84\n",
      "torch.Size([27, 3, 600, 1064]) torch.Size([27, 3]) torch.Size([27, 1, 5]) torch.Size([27])\n",
      "Prediction and Scene ID: 226\n",
      "torch.Size([5, 3, 600, 1064]) torch.Size([5, 3]) torch.Size([5, 1, 5]) torch.Size([5])\n",
      "Prediction and Scene ID: 21\n",
      "torch.Size([40, 3, 600, 1064]) torch.Size([40, 3]) torch.Size([40, 1, 5]) torch.Size([40])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction and Scene ID: 33\n",
      "torch.Size([35, 3, 600, 1064]) torch.Size([35, 3]) torch.Size([35, 1, 5]) torch.Size([35])\n",
      "Prediction and Scene ID: 199\n",
      "torch.Size([34, 3, 600, 1064]) torch.Size([34, 3]) torch.Size([34, 1, 5]) torch.Size([34])\n",
      "Prediction and Scene ID: 97\n",
      "torch.Size([51, 3, 600, 1064]) torch.Size([51, 3]) torch.Size([51, 1, 5]) torch.Size([51])\n",
      "Prediction and Scene ID: 167\n",
      "torch.Size([19, 3, 600, 1064]) torch.Size([19, 3]) torch.Size([19, 1, 5]) torch.Size([19])\n",
      "Prediction and Scene ID: 110\n",
      "torch.Size([26, 3, 600, 1064]) torch.Size([26, 3]) torch.Size([26, 1, 5]) torch.Size([26])\n",
      "Prediction and Scene ID: 144\n",
      "torch.Size([4, 3, 600, 1064]) torch.Size([4, 3]) torch.Size([4, 1, 5]) torch.Size([4])\n",
      "Prediction and Scene ID: 25\n",
      "torch.Size([10, 3, 600, 1064]) torch.Size([10, 3]) torch.Size([10, 1, 5]) torch.Size([10])\n",
      "Prediction and Scene ID: 65\n",
      "torch.Size([52, 3, 600, 1064]) torch.Size([52, 3]) torch.Size([52, 1, 5]) torch.Size([52])\n",
      "Prediction and Scene ID: 24\n",
      "torch.Size([12, 3, 600, 1064]) torch.Size([12, 3]) torch.Size([12, 1, 5]) torch.Size([12])\n",
      "Prediction and Scene ID: 197\n",
      "torch.Size([41, 3, 600, 1064]) torch.Size([41, 3]) torch.Size([41, 1, 5]) torch.Size([41])\n",
      "Prediction and Scene ID: 90\n",
      "torch.Size([13, 3, 600, 1064]) torch.Size([13, 3]) torch.Size([13, 1, 5]) torch.Size([13])\n",
      "Prediction and Scene ID: 138\n",
      "torch.Size([16, 3, 600, 1064]) torch.Size([16, 3]) torch.Size([16, 1, 5]) torch.Size([16])\n",
      "Prediction and Scene ID: 177\n",
      "torch.Size([27, 3, 600, 1064]) torch.Size([27, 3]) torch.Size([27, 1, 5]) torch.Size([27])\n",
      "Prediction and Scene ID: 156\n",
      "torch.Size([47, 3, 600, 1064]) torch.Size([47, 3]) torch.Size([47, 1, 5]) torch.Size([47])\n",
      "Prediction and Scene ID: 155\n",
      "torch.Size([39, 3, 600, 1064]) torch.Size([39, 3]) torch.Size([39, 1, 5]) torch.Size([39])\n",
      "Prediction and Scene ID: 49\n",
      "torch.Size([38, 3, 600, 1064]) torch.Size([38, 3]) torch.Size([38, 1, 5]) torch.Size([38])\n",
      "Prediction and Scene ID: 174\n",
      "torch.Size([9, 3, 600, 1064]) torch.Size([9, 3]) torch.Size([9, 1, 5]) torch.Size([9])\n",
      "Prediction and Scene ID: 134\n",
      "torch.Size([53, 3, 600, 1064]) torch.Size([53, 3]) torch.Size([53, 1, 5]) torch.Size([53])\n",
      "Prediction and Scene ID: 53\n",
      "torch.Size([47, 3, 600, 1064]) torch.Size([47, 3]) torch.Size([47, 1, 5]) torch.Size([47])\n",
      "Prediction and Scene ID: 106\n",
      "torch.Size([14, 3, 600, 1064]) torch.Size([14, 3]) torch.Size([14, 1, 5]) torch.Size([14])\n",
      "Prediction and Scene ID: 171\n",
      "torch.Size([31, 3, 600, 1064]) torch.Size([31, 3]) torch.Size([31, 1, 5]) torch.Size([31])\n",
      "Prediction and Scene ID: 55\n",
      "torch.Size([47, 3, 600, 1064]) torch.Size([47, 3]) torch.Size([47, 1, 5]) torch.Size([47])\n",
      "Prediction and Scene ID: 107\n",
      "torch.Size([5, 3, 600, 1064]) torch.Size([5, 3]) torch.Size([5, 1, 5]) torch.Size([5])\n",
      "Prediction and Scene ID: 54\n",
      "torch.Size([40, 3, 600, 1064]) torch.Size([40, 3]) torch.Size([40, 1, 5]) torch.Size([40])\n",
      "Prediction and Scene ID: 164\n",
      "torch.Size([35, 3, 600, 1064]) torch.Size([35, 3]) torch.Size([35, 1, 5]) torch.Size([35])\n",
      "Prediction and Scene ID: 190\n",
      "torch.Size([36, 3, 600, 1064]) torch.Size([36, 3]) torch.Size([36, 1, 5]) torch.Size([36])\n",
      "Prediction and Scene ID: 52\n",
      "torch.Size([13, 3, 600, 1064]) torch.Size([13, 3]) torch.Size([13, 1, 5]) torch.Size([13])\n",
      "Prediction and Scene ID: 176\n",
      "torch.Size([6, 3, 600, 1064]) torch.Size([6, 3]) torch.Size([6, 1, 5]) torch.Size([6])\n",
      "Prediction and Scene ID: 210\n",
      "torch.Size([6, 3, 600, 1064]) torch.Size([6, 3]) torch.Size([6, 1, 5]) torch.Size([6])\n",
      "Prediction and Scene ID: 140\n",
      "torch.Size([42, 3, 600, 1064]) torch.Size([42, 3]) torch.Size([42, 1, 5]) torch.Size([42])\n",
      "Prediction and Scene ID: 168\n",
      "torch.Size([29, 3, 600, 1064]) torch.Size([29, 3]) torch.Size([29, 1, 5]) torch.Size([29])\n",
      "Prediction and Scene ID: 67\n",
      "torch.Size([39, 3, 600, 1064]) torch.Size([39, 3]) torch.Size([39, 1, 5]) torch.Size([39])\n",
      "Prediction and Scene ID: 228\n",
      "torch.Size([35, 3, 600, 1064]) torch.Size([35, 3]) torch.Size([35, 1, 5]) torch.Size([35])\n",
      "Prediction and Scene ID: 186\n",
      "torch.Size([52, 3, 600, 1064]) torch.Size([52, 3]) torch.Size([52, 1, 5]) torch.Size([52])\n",
      "Prediction and Scene ID: 10\n",
      "torch.Size([20, 3, 600, 1064]) torch.Size([20, 3]) torch.Size([20, 1, 5]) torch.Size([20])\n",
      "Prediction and Scene ID: 8\n",
      "torch.Size([51, 3, 600, 1064]) torch.Size([51, 3]) torch.Size([51, 1, 5]) torch.Size([51])\n",
      "Prediction and Scene ID: 12\n",
      "torch.Size([56, 3, 600, 1064]) torch.Size([56, 3]) torch.Size([56, 1, 5]) torch.Size([56])\n",
      "Prediction and Scene ID: 146\n",
      "torch.Size([30, 3, 600, 1064]) torch.Size([30, 3]) torch.Size([30, 1, 5]) torch.Size([30])\n",
      "Prediction and Scene ID: 219\n",
      "torch.Size([49, 3, 600, 1064]) torch.Size([49, 3]) torch.Size([49, 1, 5]) torch.Size([49])\n",
      "Prediction and Scene ID: 29\n",
      "torch.Size([57, 3, 600, 1064]) torch.Size([57, 3]) torch.Size([57, 1, 5]) torch.Size([57])\n",
      "Prediction and Scene ID: 150\n",
      "torch.Size([18, 3, 600, 1064]) torch.Size([18, 3]) torch.Size([18, 1, 5]) torch.Size([18])\n",
      "Prediction and Scene ID: 125\n",
      "torch.Size([37, 3, 600, 1064]) torch.Size([37, 3]) torch.Size([37, 1, 5]) torch.Size([37])\n",
      "Prediction and Scene ID: 218\n",
      "torch.Size([56, 3, 600, 1064]) torch.Size([56, 3]) torch.Size([56, 1, 5]) torch.Size([56])\n",
      "Prediction and Scene ID: 71\n",
      "torch.Size([18, 3, 600, 1064]) torch.Size([18, 3]) torch.Size([18, 1, 5]) torch.Size([18])\n",
      "Prediction and Scene ID: 7\n",
      "torch.Size([11, 3, 600, 1064]) torch.Size([11, 3]) torch.Size([11, 1, 5]) torch.Size([11])\n",
      "Prediction and Scene ID: 220\n",
      "torch.Size([26, 3, 600, 1064]) torch.Size([26, 3]) torch.Size([26, 1, 5]) torch.Size([26])\n",
      "Prediction and Scene ID: 205\n",
      "torch.Size([37, 3, 600, 1064]) torch.Size([37, 3]) torch.Size([37, 1, 5]) torch.Size([37])\n",
      "Prediction and Scene ID: 9\n",
      "torch.Size([37, 3, 600, 1064]) torch.Size([37, 3]) torch.Size([37, 1, 5]) torch.Size([37])\n",
      "Prediction and Scene ID: 59\n",
      "torch.Size([52, 3, 600, 1064]) torch.Size([52, 3]) torch.Size([52, 1, 5]) torch.Size([52])\n",
      "Prediction and Scene ID: 118\n",
      "torch.Size([23, 3, 600, 1064]) torch.Size([23, 3]) torch.Size([23, 1, 5]) torch.Size([23])\n",
      "Prediction and Scene ID: 116\n",
      "torch.Size([49, 3, 600, 1064]) torch.Size([49, 3]) torch.Size([49, 1, 5]) torch.Size([49])\n",
      "Prediction and Scene ID: 187\n",
      "torch.Size([37, 3, 600, 1064]) torch.Size([37, 3]) torch.Size([37, 1, 5]) torch.Size([37])\n",
      "Prediction and Scene ID: 83\n",
      "torch.Size([17, 3, 600, 1064]) torch.Size([17, 3]) torch.Size([17, 1, 5]) torch.Size([17])\n",
      "Prediction and Scene ID: 158\n",
      "torch.Size([36, 3, 600, 1064]) torch.Size([36, 3]) torch.Size([36, 1, 5]) torch.Size([36])\n",
      "Prediction and Scene ID: 191\n",
      "torch.Size([35, 3, 600, 1064]) torch.Size([35, 3]) torch.Size([35, 1, 5]) torch.Size([35])\n",
      "Prediction and Scene ID: 64\n",
      "torch.Size([35, 3, 600, 1064]) torch.Size([35, 3]) torch.Size([35, 1, 5]) torch.Size([35])\n",
      "Prediction and Scene ID: 46\n",
      "torch.Size([20, 3, 600, 1064]) torch.Size([20, 3]) torch.Size([20, 1, 5]) torch.Size([20])\n",
      "Prediction and Scene ID: 30\n",
      "torch.Size([59, 3, 600, 1064]) torch.Size([59, 3]) torch.Size([59, 1, 5]) torch.Size([59])\n",
      "Prediction and Scene ID: 207\n",
      "torch.Size([9, 3, 600, 1064]) torch.Size([9, 3]) torch.Size([9, 1, 5]) torch.Size([9])\n",
      "Prediction and Scene ID: 47\n",
      "torch.Size([16, 3, 600, 1064]) torch.Size([16, 3]) torch.Size([16, 1, 5]) torch.Size([16])\n",
      "Prediction and Scene ID: 200\n",
      "torch.Size([31, 3, 600, 1064]) torch.Size([31, 3]) torch.Size([31, 1, 5]) torch.Size([31])\n",
      "Prediction and Scene ID: 58\n",
      "torch.Size([31, 3, 600, 1064]) torch.Size([31, 3]) torch.Size([31, 1, 5]) torch.Size([31])\n",
      "Prediction and Scene ID: 178\n",
      "torch.Size([29, 3, 600, 1064]) torch.Size([29, 3]) torch.Size([29, 1, 5]) torch.Size([29])\n",
      "Prediction and Scene ID: 198\n",
      "torch.Size([17, 3, 600, 1064]) torch.Size([17, 3]) torch.Size([17, 1, 5]) torch.Size([17])\n",
      "Prediction and Scene ID: 195\n",
      "torch.Size([13, 3, 600, 1064]) torch.Size([13, 3]) torch.Size([13, 1, 5]) torch.Size([13])\n",
      "Prediction and Scene ID: 202\n",
      "torch.Size([31, 3, 600, 1064]) torch.Size([31, 3]) torch.Size([31, 1, 5]) torch.Size([31])\n",
      "Prediction and Scene ID: 173\n",
      "torch.Size([5, 3, 600, 1064]) torch.Size([5, 3]) torch.Size([5, 1, 5]) torch.Size([5])\n",
      "Prediction and Scene ID: 194\n",
      "torch.Size([20, 3, 600, 1064]) torch.Size([20, 3]) torch.Size([20, 1, 5]) torch.Size([20])\n",
      "Prediction and Scene ID: 104\n",
      "torch.Size([9, 3, 600, 1064]) torch.Size([9, 3]) torch.Size([9, 1, 5]) torch.Size([9])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction and Scene ID: 153\n",
      "torch.Size([61, 3, 600, 1064]) torch.Size([61, 3]) torch.Size([61, 1, 5]) torch.Size([61])\n",
      "Prediction and Scene ID: 41\n",
      "torch.Size([50, 3, 600, 1064]) torch.Size([50, 3]) torch.Size([50, 1, 5]) torch.Size([50])\n",
      "Prediction and Scene ID: 80\n",
      "torch.Size([13, 3, 600, 1064]) torch.Size([13, 3]) torch.Size([13, 1, 5]) torch.Size([13])\n",
      "Prediction and Scene ID: 32\n",
      "torch.Size([32, 3, 600, 1064]) torch.Size([32, 3]) torch.Size([32, 1, 5]) torch.Size([32])\n",
      "Prediction and Scene ID: 133\n",
      "torch.Size([21, 3, 600, 1064]) torch.Size([21, 3]) torch.Size([21, 1, 5]) torch.Size([21])\n",
      "Prediction and Scene ID: 152\n",
      "torch.Size([19, 3, 600, 1064]) torch.Size([19, 3]) torch.Size([19, 1, 5]) torch.Size([19])\n",
      "Prediction and Scene ID: 180\n",
      "torch.Size([24, 3, 600, 1064]) torch.Size([24, 3]) torch.Size([24, 1, 5]) torch.Size([24])\n",
      "Prediction and Scene ID: 145\n",
      "torch.Size([28, 3, 600, 1064]) torch.Size([28, 3]) torch.Size([28, 1, 5]) torch.Size([28])\n",
      "Prediction and Scene ID: 183\n",
      "torch.Size([34, 3, 600, 1064]) torch.Size([34, 3]) torch.Size([34, 1, 5]) torch.Size([34])\n",
      "Prediction and Scene ID: 162\n",
      "torch.Size([9, 3, 600, 1064]) torch.Size([9, 3]) torch.Size([9, 1, 5]) torch.Size([9])\n",
      "Prediction and Scene ID: 73\n",
      "torch.Size([53, 3, 600, 1064]) torch.Size([53, 3]) torch.Size([53, 1, 5]) torch.Size([53])\n",
      "Prediction and Scene ID: 126\n",
      "torch.Size([30, 3, 600, 1064]) torch.Size([30, 3]) torch.Size([30, 1, 5]) torch.Size([30])\n",
      "Prediction and Scene ID: 129\n",
      "torch.Size([15, 3, 600, 1064]) torch.Size([15, 3]) torch.Size([15, 1, 5]) torch.Size([15])\n",
      "Prediction and Scene ID: 147\n",
      "torch.Size([28, 3, 600, 1064]) torch.Size([28, 3]) torch.Size([28, 1, 5]) torch.Size([28])\n",
      "Prediction and Scene ID: 16\n",
      "torch.Size([55, 3, 600, 1064]) torch.Size([55, 3]) torch.Size([55, 1, 5]) torch.Size([55])\n",
      "Prediction and Scene ID: 181\n",
      "torch.Size([33, 3, 600, 1064]) torch.Size([33, 3]) torch.Size([33, 1, 5]) torch.Size([33])\n",
      "Prediction and Scene ID: 175\n",
      "torch.Size([38, 3, 600, 1064]) torch.Size([38, 3]) torch.Size([38, 1, 5]) torch.Size([38])\n",
      "Prediction and Scene ID: 22\n",
      "torch.Size([37, 3, 600, 1064]) torch.Size([37, 3]) torch.Size([37, 1, 5]) torch.Size([37])\n",
      "Prediction and Scene ID: 70\n",
      "torch.Size([54, 3, 600, 1064]) torch.Size([54, 3]) torch.Size([54, 1, 5]) torch.Size([54])\n",
      "Prediction and Scene ID: 131\n",
      "torch.Size([17, 3, 600, 1064]) torch.Size([17, 3]) torch.Size([17, 1, 5]) torch.Size([17])\n",
      "Prediction and Scene ID: 141\n",
      "torch.Size([53, 3, 600, 1064]) torch.Size([53, 3]) torch.Size([53, 1, 5]) torch.Size([53])\n",
      "Prediction and Scene ID: 20\n",
      "torch.Size([59, 3, 600, 1064]) torch.Size([59, 3]) torch.Size([59, 1, 5]) torch.Size([59])\n",
      "Prediction and Scene ID: 51\n",
      "torch.Size([21, 3, 600, 1064]) torch.Size([21, 3]) torch.Size([21, 1, 5]) torch.Size([21])\n",
      "Prediction and Scene ID: 74\n",
      "torch.Size([32, 3, 600, 1064]) torch.Size([32, 3]) torch.Size([32, 1, 5]) torch.Size([32])\n",
      "Prediction and Scene ID: 43\n",
      "torch.Size([19, 3, 600, 1064]) torch.Size([19, 3]) torch.Size([19, 1, 5]) torch.Size([19])\n",
      "Prediction and Scene ID: 212\n",
      "torch.Size([17, 3, 600, 1064]) torch.Size([17, 3]) torch.Size([17, 1, 5]) torch.Size([17])\n",
      "Prediction and Scene ID: 88\n",
      "torch.Size([11, 3, 600, 1064]) torch.Size([11, 3]) torch.Size([11, 1, 5]) torch.Size([11])\n",
      "Prediction and Scene ID: 62\n",
      "torch.Size([21, 3, 600, 1064]) torch.Size([21, 3]) torch.Size([21, 1, 5]) torch.Size([21])\n",
      "Prediction and Scene ID: 204\n",
      "torch.Size([48, 3, 600, 1064]) torch.Size([48, 3]) torch.Size([48, 1, 5]) torch.Size([48])\n",
      "Prediction and Scene ID: 170\n",
      "torch.Size([11, 3, 600, 1064]) torch.Size([11, 3]) torch.Size([11, 1, 5]) torch.Size([11])\n",
      "Prediction and Scene ID: 93\n",
      "torch.Size([4, 3, 600, 1064]) torch.Size([4, 3]) torch.Size([4, 1, 5]) torch.Size([4])\n",
      "Prediction and Scene ID: 119\n",
      "torch.Size([54, 3, 600, 1064]) torch.Size([54, 3]) torch.Size([54, 1, 5]) torch.Size([54])\n",
      "Prediction and Scene ID: 224\n",
      "torch.Size([38, 3, 600, 1064]) torch.Size([38, 3]) torch.Size([38, 1, 5]) torch.Size([38])\n",
      "Prediction and Scene ID: 101\n",
      "torch.Size([23, 3, 600, 1064]) torch.Size([23, 3]) torch.Size([23, 1, 5]) torch.Size([23])\n",
      "Prediction and Scene ID: 172\n",
      "torch.Size([22, 3, 600, 1064]) torch.Size([22, 3]) torch.Size([22, 1, 5]) torch.Size([22])\n",
      "Prediction and Scene ID: 36\n",
      "torch.Size([26, 3, 600, 1064]) torch.Size([26, 3]) torch.Size([26, 1, 5]) torch.Size([26])\n",
      "Prediction and Scene ID: 26\n",
      "torch.Size([13, 3, 600, 1064]) torch.Size([13, 3]) torch.Size([13, 1, 5]) torch.Size([13])\n",
      "Prediction and Scene ID: 112\n",
      "torch.Size([62, 3, 600, 1064]) torch.Size([62, 3]) torch.Size([62, 1, 5]) torch.Size([62])\n",
      "Prediction and Scene ID: 4\n",
      "torch.Size([7, 3, 600, 1064]) torch.Size([7, 3]) torch.Size([7, 1, 5]) torch.Size([7])\n",
      "Prediction and Scene ID: 5\n",
      "torch.Size([18, 3, 600, 1064]) torch.Size([18, 3]) torch.Size([18, 1, 5]) torch.Size([18])\n",
      "Prediction and Scene ID: 182\n",
      "torch.Size([29, 3, 600, 1064]) torch.Size([29, 3]) torch.Size([29, 1, 5]) torch.Size([29])\n",
      "Prediction and Scene ID: 130\n",
      "torch.Size([30, 3, 600, 1064]) torch.Size([30, 3]) torch.Size([30, 1, 5]) torch.Size([30])\n",
      "Prediction and Scene ID: 139\n",
      "torch.Size([23, 3, 600, 1064]) torch.Size([23, 3]) torch.Size([23, 1, 5]) torch.Size([23])\n",
      "Prediction and Scene ID: 37\n",
      "torch.Size([41, 3, 600, 1064]) torch.Size([41, 3]) torch.Size([41, 1, 5]) torch.Size([41])\n",
      "Prediction and Scene ID: 13\n",
      "torch.Size([44, 3, 600, 1064]) torch.Size([44, 3]) torch.Size([44, 1, 5]) torch.Size([44])\n",
      "Prediction and Scene ID: 179\n",
      "torch.Size([41, 3, 600, 1064]) torch.Size([41, 3]) torch.Size([41, 1, 5]) torch.Size([41])\n",
      "Prediction and Scene ID: 100\n",
      "torch.Size([18, 3, 600, 1064]) torch.Size([18, 3]) torch.Size([18, 1, 5]) torch.Size([18])\n",
      "Prediction and Scene ID: 42\n",
      "torch.Size([12, 3, 600, 1064]) torch.Size([12, 3]) torch.Size([12, 1, 5]) torch.Size([12])\n"
     ]
    }
   ],
   "source": [
    "def makeSTTranPrediction(model, data):\n",
    "    im_data = copy.deepcopy(data[0].cuda(0))\n",
    "    im_info = copy.deepcopy(data[1].cuda(0))\n",
    "    gt_boxes = copy.deepcopy(data[2].cuda(0))\n",
    "    num_boxes = copy.deepcopy(data[3].cuda(0))\n",
    "    index = data[4]\n",
    "    scene_id = data[5]\n",
    "    print(f\"Prediction and Scene ID: {scene_id}\")\n",
    "    print(im_data.shape, im_info.shape, gt_boxes.shape, num_boxes.shape)\n",
    "    gt_annotation = torch.zeros([im_data.shape[0], 1, 5])\n",
    "    \n",
    "    entry = object_detector(im_data, im_info, gt_boxes, num_boxes, gt_annotation, im_all=None)\n",
    "    pred = model(entry)\n",
    "    return entry, pred, scene_id\n",
    "\n",
    "predictions = {}\n",
    "obj_entries = {}\n",
    "failed_to_load_scenes = []\n",
    "with torch.no_grad():\n",
    "    for b, data in enumerate(mg_dataloader):\n",
    "        try:\n",
    "#         if b >29:\n",
    "            obj_entry, pred, scene_id = makeSTTranPrediction(model, data)\n",
    "            predictions[scene_id] = pred\n",
    "            obj_entries[scene_id] = obj_entry\n",
    "        except:\n",
    "            failed_to_load_scenes.append(data[5])\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "inner-maldives",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STTran failed to make scene predictions on the following scene ids:\n",
      "\n",
      "[95, 76, 87, 34, 206, 44, 111, 86, 31, 211, 91, 19, 151, 45, 201, 166, 188, 3, 6, 148, 84, 72, 123, 122, 21, 33, 199, 97, 167, 110, 57, 60, 65, 120, 197, 50, 90, 56, 138, 177, 223, 156, 155, 49, 174, 134, 53, 106, 171, 55, 54, 164, 190, 52, 77, 140, 168, 67, 228, 186, 10, 8, 12, 146, 75, 219, 29, 150, 108, 125, 218, 71, 7, 220, 38, 205, 9, 59, 118, 116, 187, 83, 222, 184, 158, 191, 64, 46, 225, 30, 207, 47, 200, 58, 178, 198, 195, 202, 194, 104, 153, 41, 80, 32, 133, 152, 180, 145, 124, 183, 63, 162, 73, 126, 129, 89, 147, 16, 181, 175, 61, 22, 227, 70, 131, 141, 20, 51, 74, 43, 212, 88, 62, 204, 170, 119, 224, 101, 172, 36, 26, 112, 5, 182, 136, 130, 139, 37, 13, 179, 100, 42]\n"
     ]
    }
   ],
   "source": [
    "print(\"STTran failed to make scene predictions on the following scene ids:\\n\")\n",
    "print(failed_to_load_scenes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "competent-sport",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully predicted 60 scenes\n",
      "Failed to predict 152 scenes\n"
     ]
    }
   ],
   "source": [
    "print(f\"Succesfully predicted {len(predictions.keys())} scenes\")\n",
    "print(f\"Failed to predict {len(failed_to_load_scenes)} scenes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "coupled-dimension",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of the 60 predictions, 56 have a matching annotation from MovieGraph\n"
     ]
    }
   ],
   "source": [
    "match_count = 0\n",
    "for p in predictions.keys():\n",
    "    if p in annotations.keys():\n",
    "        match_count += 1\n",
    "print(f\"Of the {len(predictions.keys())} predictions, {match_count} have a matching annotation from MovieGraph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-house",
   "metadata": {},
   "source": [
    "## Convert Tensor Predictions to Readable Triplets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "explicit-causing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This scene is made of of 23 frames and each frame's scene graph is available\n",
      "Below are the triplets of each frame\n",
      "\n",
      "[['person', 'wearing', 'clothes'], ['clothes', 'in', 'person'], ['person', 'touching', 'clothes'], ['person', 'not_looking_at', 'clothes'], ['person', 'unsure', 'clothes'], ['person', 'looking_at', 'clothes'], ['person', 'carrying', 'clothes'], ['person', 'covered_by', 'clothes'], ['clothes', 'behind', 'person'], ['person', 'holding', 'clothes'], ['person', 'leaning_on', 'clothes'], ['person', 'not_contacting', 'clothes'], ['clothes', 'beneath', 'person'], ['person', 'standing_on', 'clothes'], ['person', 'sitting_on', 'clothes'], ['person', 'other_relationship', 'clothes'], ['person', 'drinking_from', 'clothes'], ['clothes', 'on_the_side_of', 'person'], ['person', 'lying_on', 'clothes'], ['clothes', 'above', 'person'], ['person', 'have_it_on_the_back', 'clothes'], ['person', 'eating', 'clothes'], ['person', 'wiping', 'clothes'], ['person', 'twisting', 'clothes'], ['person', 'writing_on', 'clothes'], ['clothes', 'in_front_of', 'person'], ['person', 'wiping', 'clothes'], ['person', 'on_the_side_of', 'clothes'], ['person', 'leaning_on', 'clothes'], ['person', 'lying_on', 'clothes'], ['person', 'not_contacting', 'clothes'], ['person', 'in', 'clothes'], ['person', 'above', 'clothes'], ['person', 'behind', 'clothes'], ['person', 'in_front_of', 'clothes'], ['person', 'beneath', 'clothes'], ['person', 'unsure', 'clothes'], ['person', 'writing_on', 'clothes'], ['person', 'holding', 'clothes'], ['clothes', 'eating', 'person'], ['person', 'eating', 'clothes'], ['clothes', 'leaning_on', 'person'], ['person', 'sitting_on', 'clothes'], ['clothes', 'carrying', 'person'], ['clothes', 'covered_by', 'person'], ['clothes', 'drinking_from', 'person'], ['person', 'other_relationship', 'clothes'], ['clothes', 'have_it_on_the_back', 'person'], ['clothes', 'holding', 'person'], ['clothes', 'lying_on', 'person'], ['person', 'drinking_from', 'clothes'], ['clothes', 'not_contacting', 'person'], ['clothes', 'other_relationship', 'person'], ['clothes', 'sitting_on', 'person'], ['clothes', 'standing_on', 'person'], ['clothes', 'touching', 'person'], ['clothes', 'twisting', 'person'], ['clothes', 'wearing', 'person'], ['person', 'standing_on', 'clothes'], ['person', 'touching', 'clothes'], ['person', 'twisting', 'clothes'], ['person', 'wearing', 'clothes'], ['person', 'covered_by', 'clothes'], ['person', 'carrying', 'clothes'], ['person', 'in', 'clothes'], ['person', 'on_the_side_of', 'clothes'], ['person', 'behind', 'clothes'], ['person', 'in_front_of', 'clothes'], ['person', 'beneath', 'clothes'], ['person', 'above', 'clothes'], ['person', 'not_looking_at', 'clothes'], ['person', 'looking_at', 'clothes'], ['clothes', 'writing_on', 'person'], ['clothes', 'wiping', 'person'], ['clothes', 'looking_at', 'person'], ['clothes', 'not_looking_at', 'person'], ['clothes', 'unsure', 'person'], ['person', 'have_it_on_the_back', 'clothes']]\n"
     ]
    }
   ],
   "source": [
    "def tripletsToWords(triplets, object_classes, relationship_classes):\n",
    "    triplet_words = []\n",
    "    for sub1, rel, sub2 in triplets:\n",
    "        triplet_words.append([object_classes[sub1], relationship_classes[rel],object_classes[sub2]])\n",
    "    return triplet_words\n",
    "\n",
    "evaluator = MGSceneGraphEvaluator(\n",
    "    mode=mode,\n",
    "    AG_object_classes=MG_dataset.object_classes,\n",
    "    AG_all_predicates=MG_dataset.relationship_classes,\n",
    "    AG_attention_predicates=MG_dataset.attention_relationships,\n",
    "    AG_spatial_predicates=MG_dataset.spatial_relationships,\n",
    "    AG_contacting_predicates=MG_dataset.contacting_relationships,\n",
    "    iou_threshold=0.5,\n",
    "    constraint='no')\n",
    "\n",
    "pred = predictions[14]\n",
    "triplets_by_frame = evaluator.evaluate_scene_graph(pred)\n",
    "print(f\"This scene is made of of {len(triplets_by_frame)} frames and each frame's scene graph is available\\nBelow are the triplets of each frame\\n\")\n",
    "print(evaluator.pred_to_word_triplets(pred)[5]) # print word triplets of frame 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "north-blade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Prediction. Each key corresponds to a scene number: \n",
      "dict_keys([18, 14, 216, 163, 217, 213, 160, 11, 165, 103, 68, 221, 154, 209, 35, 28, 117, 214, 98, 203, 193, 137, 23, 66, 121, 185, 192, 132, 157, 78, 109, 102, 99, 127, 114, 161, 113, 196, 15, 27, 143, 159, 17, 169, 40, 69, 215, 48, 115, 149, 226, 144, 25, 24, 107, 176, 210, 173, 93, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\"Available Prediction. Each key corresponds to a scene number: \")\n",
    "print(predictions.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "southwest-restaurant",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-18-ae6fafba0f1f>, line 26)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-ae6fafba0f1f>\"\u001b[0;36m, line \u001b[0;32m26\u001b[0m\n\u001b[0;31m    if !sub1_pred_only:\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def get_time_range(timestamp_range):\n",
    "    # Split the timestamp range into start and end times\n",
    "    # A timestamp_range is the to reference the mini scenes of a scenes of the gt annotations\n",
    "    start_time, end_time = timestamp_range.split('-')\n",
    "    start_time_min, start_time_sec = start_time.split(':')\n",
    "    end_time_min, end_time_sec = end_time.split(':')\n",
    "    \n",
    "    start = int((int(start_time_min)*60) + (float(start_time_sec)))\n",
    "    end = int((int(end_time_min)*60) + (float(end_time_sec)))\n",
    "    \n",
    "    return start, end\n",
    "\n",
    "def get_unique_triplets(lst):\n",
    "    unique_triplets = set()\n",
    "    for sublist in lst:\n",
    "        for i in range(len(sublist) - 2):\n",
    "            triplet = tuple(sublist[i:i+3])\n",
    "            unique_triplets.add(triplet)\n",
    "    unique_triplets_list = [list(t) for t in unique_triplets]\n",
    "    return unique_triplets_list\n",
    "\n",
    "def evaluatateOneFramePredicationWithAnnotations(predictions, annotations, sub1_pred_only = False):\n",
    "    #if sub1_pred_only = True, the comparison returns as a match if <sub1-pred> = <sub1-pred> basically we are ignoring the third part of the triplet\n",
    "    total = len(annotations)\n",
    "    matches = 0\n",
    "    if sub1_pred_only:\n",
    "        #haha get it? triplet = 3, biplet = 2 \n",
    "        triplet_to_biplet =[]\n",
    "        for p in predictions:\n",
    "            triplet_to_biplet.append(p[:2])\n",
    "        for a in annotations:\n",
    "            a = a[:2]\n",
    "            if a in triplet_to_biplet:\n",
    "                matches+= 1\n",
    "    else:\n",
    "        for a in annotations:\n",
    "            if a in predictions:\n",
    "                matches += 1\n",
    "    return matches, total\n",
    "\n",
    "def evaluateScenePredictionWithAnnotation(prediction, gt_annotation, offset=0, sub1_pred_only = False):\n",
    "    total_clip_triplet_matches = 0\n",
    "    total_clip_gt_triplets = 0\n",
    "    \n",
    "    for time_interval in gt_annotation.keys():\n",
    "        start_frame, end_frame =  get_time_range(time_interval)\n",
    "        list_of_tripelts_over_frame_range = []\n",
    "        for fr in range(start_frame+offset, end_frame+offset+1):\n",
    "            if fr < len(pred_word_triplets_by_frame):\n",
    "#                 print(type(list_of_tripelts_over_frame_range), type(pred_word_triplets_by_frame[fr]))\n",
    "#                 print()\n",
    "                list_of_tripelts_over_frame_range = list_of_tripelts_over_frame_range + pred_word_triplets_by_frame[fr]\n",
    "#                 list_of_tripelts_over_frame_range = list_of_tripelts_over_frame_range + pred_word_triplets_by_frame[fr]\n",
    "        union_fr_trips = get_unique_triplets(list_of_tripelts_over_frame_range)\n",
    "\n",
    "        # ms_matches is the number of triplet matches in the mini scene\n",
    "        # ms_total_gt_triplets is the total numbers of gt triplets in the mini scene\n",
    "        ms_matches, ms_total_gt_triplets =  evaluatateOneFramePredicationWithAnnotations(union_fr_trips, gt_annotation[time_interval], sub1_pred_only = sub1_pred_only)\n",
    "        total_clip_triplet_matches += ms_matches\n",
    "        total_clip_gt_triplets += ms_total_gt_triplets\n",
    "    return total_clip_triplet_matches, total_clip_gt_triplets\n",
    "    \n",
    "\n",
    "succesful = 0\n",
    "failed = 0\n",
    "total_matched_clips = 0\n",
    "total_clips = 0\n",
    "print(predictions.keys())\n",
    "offset = 1 #shifts the timestamps of the annotations up relative to the frame splits. I use 1 because I believe the frame constructions are 1 second ahead of the annotations\n",
    "for i, key in enumerate(predictions.keys()):\n",
    "    print(f\"Working on scene {key}:\")\n",
    "    try:\n",
    "        pred_word_triplets_by_frame = evaluator.pred_to_word_triplets(predictions[key])\n",
    "        if key in annotations.keys():\n",
    "            mg_annot = annotations.getAnnotation(key)\n",
    "            total_clip_triplet_matches, total_clip_gt_triplets = evaluateScenePredictionWithAnnotation(\n",
    "                pred_word_triplets_by_frame, mg_annot, offset = offset)\n",
    "            print(f\"\\tIn scene {key} we match to {total_clip_triplet_matches} out of the {total_clip_gt_triplets} GT Movie Graph annotations\")\n",
    "            succesful += 1\n",
    "            total_matched_clips += total_clip_triplet_matches\n",
    "            total_clips += total_clip_gt_triplets\n",
    "    except:\n",
    "        print(\"\\t This Scene Failed\")\n",
    "        failed += 1\n",
    "        continue\n",
    "print(f\"\\n\\n{succesful} scene evaluations succeded and {failed} failed. Between all clips, themodel acheives a recall score of {total_matched_clips} out of {total_clips} clips.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "universal-terrace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a', 'b'], ['d', 'e']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [['a','b','c'], ['d', 'e', 'f']]\n",
    "new = []\n",
    "for t in test:\n",
    "    new.append(t[:2])\n",
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-quebec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
